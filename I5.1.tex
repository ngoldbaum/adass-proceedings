% This is the ADASS_template.tex LaTeX file, 26th August 2016.
% It is based on the ASP general author template file, but modified to reflect the specific
% requirements of the ADASS proceedings.
% Copyright 2014, Astronomical Society of the Pacific Conference Series
% Revision:  14 August 2014

% To compile, at the command line positioned at this folder, type:
% latex ADASS_template
% latex ADASS_template
% dvipdfm ADASS_template
% This will create a file called aspauthor.pdf.}

\documentclass[11pt,twoside]{article}


% Do NOT use ANY packages other than asp2014. 
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

% References must all use BibTeX entries in a .bibfile.
% References must be cited in the text using \citet{} or \citep{}.
% Do not use \cite{}.
% See ManuscriptInstructions.pdf for more details
\bibliographystyle{asp2014}

% The ``markboth'' line sets up the running heads for the paper.
% 1 author: "Surname"
% 2 authors: "Surname1 and Surname2"
% 3 authors: "Surname1, Surname2, and Surname3"
% >3 authors: "Surname1 et al."
% Replace ``Short Title'' with the actual paper title, shortened if necessary.
% Use mixed case type for the shortened title
% Ensure shortened title does not cause an overfull hbox LaTeX error
% See ASPmanual2010.pdf 2.1.4  and ManuscriptInstructions.pdf for more details
\markboth{Goldbaum}{Extracting Insights from Astrophysics Simulations}

\begin{document}

\title{Extracting Insights from Astrophysics Simulations}

% Note the position of the comma between the author name and the 
% affiliation number.
% Author names should be separated by commas.
% The final author should be preceded by "and".
% Affiliations should not be repeated across multiple \affil commands. If several
% authors share an affiliation this should be in a single \affil which can then
% be referenced for several author names.
% See ManuscriptInstructions.pdf and ASPmanual2010.pdf 3.1.4 for more details
\author{Nathan~J.~Goldbaum$^1$}
\affil{$^1$National Center for Supercomputing Applications, University of
  Illinois at Urbana-Champaign, Urbana, IL, USA; \email{ngoldbau@illinois.edu}}

% This section is for ADS Processing.  There must be one line per author.
\paperauthor{Nathan~J.~Goldbaum}{ngoldbau@illinois.edu}{0000-0001-5557-267X}{University
of Illinois at Urbana-Champaign}{National Center for Supercomputing Applications}{Urbana}{IL}{61801}{USA}


\begin{abstract}
Simulations inform all aspects of modern astrophysical research, ranging in scale from 1D and 2D test problems that can run in seconds on an astronomer's laptop all the way to large-scale 3D calculations that run on the largest supercomputers, with a spectrum of data sizes and shapes filling the landscape between these two extremes. I review the diversity of astrophysics simulation data formats commonly in use by researchers, providing an overview of the most common simulation techniques, including pure N-body dynamics, smoothed particle hydrodynamics (SPH), adaptive mesh refinement (AMR), and unstructured meshes. Additionally, I highlight methods for incorporating physical phenomena that are important for astrophysics, including chemistry, magnetic fields, radiative transport, and "subgrid" recipes for important physics that cannot be directly resolved in a simulation. In addition to the numerical techniques, I also discuss the communities that have developed around these simulation codes and argue that increasing use and availability of open community codes has dramatically lowered the barrier to entry for novice simulators.
\end{abstract}

\section{Introduction}
Direct 3D numerical simulations of astrophysical phenomena are a key driver of theoretical prediction in 21st century astrophysics. By offering a dynamical view of phenomena that are unobservable on human timescales, numerical simulations allow researchers to extract unique insights that are unavailable from other approaches due to the complexity of the underlying physical systems. Due to the nature of the numerical discretizations driving the simulation codes, increasing computer power will naturally bring more and more phenomena within the envelope of computation feasability. Coupled with improved computational methods and modeling techniques this means nearly all fields in astrophysics are informed by direct numerical simulations at some level, and in many fields at a very high level.

It is therefore not surprising that the astrophysics simulation software ecosystem is quite diverse. Different systems lend themselves to different simulation techniques, results are validated by being reproduced from completely independent codebases, and individual research groups are incentivized to develop software both to produce new results and in some cases to make proprietary use of new technology to produce novel research results. Increasingly, open research communities are developing, producing simulation software that can be used by anyone with access to the necessary computational resources to run a simulation.

In this paper I will review the ecosystem of astrophysical simulation software and simulation techniques. This will by no means be a comprehensive review. Instead, I will primarily focus on generally available community research software. I will also mention several propietary research codes that have made a substantial impact on the research landscape in terms of published results. I will also attempt to focus on codes that are still used to generate research results, leaving older software that is no longer commonly used to historical reviews. The review will be organized according to the simulation technique used by the software. First, Section~\ref{nbody} in I will discuss pure N-body and smoothed particle hydrodynamics (SPH) codes. Next, in Section~\ref{amr} I will discuss grid-based hydrodynamics codes, with a focus on adaptive mesh refinement codes. Finally, in Section~\ref{usm} I will discuss moving mesh and advanced meshless hydrodynamics codes.

\section{N-body Methods}
\label{nbody}

In this approach, an astrophysical system is discretized into a system of self-gravitating particles which interact via their mutual gravitational attraction. In most cases the particles are simply test particles which are used to sample the gravitational force of the system, but each individual particle does not necessarily have any physical meaning. N-body methods are Lagrangian and thus well-suited for astrophysical problems that tend to have large dynamic ranges in the scale of structures. Rather than wasting computation on regions in which not much is happening, work naturally flows to where the mass is contained. N-body methods in astrophysics can be further subdivided in pure N-body methods and SPH methods, which can include the effect of gas dynamics. We describe these two approaches below.

Using pure N-body techniques to understand physical systems has a rich history \citep[see e.g.\ ][]{holmberg1941, hoerner1960, peebles1970, press1974}, in particular for astrophysical systems where the influence of gas dynamics is negligible. In some cases, such as simulations of star clusters \citep{wang2016}, or even galaxies \citep{bedorf2014}, the masses of particles in the simulation approach the masses of the physical bodies that make up the real gravitational system.

Since gravity is a long-range force, this means that the gravitational force calculation for any given particle depends on the masses and positions of every other particle in the simulation. Therefore, the worst-case naive algorithm that evaluates the pairwise gravitational force for all particles is $O(N^2)$. State-of-the-art N-body calculations include as many as a trillion particles and are run on clusters containing thousands of compute nodes --- the naive pairwise algorithm would simply not be feasible to run on simulations of this size. Instead, N-body codes make use of some form of spatial acceleration to conver the calculation of the gravitational force into an $O(N \log N)$ calculation. Often, this involves depositing the N-body data into a mesh and then making use of a fast Fourier transform (FFT) to solve the Poisson equation on the mesh, this is known as a Particle-mesh (PM) algorithm \citep{hockney1988}. The accelerations at the particle locations can then be calculated by interpolating from the accelerations at the mesh locations. Another alternative is to calculate the gravitational acceleration by making use of an octree data structure. Particles contained in octree zones that are sufficiently far away (usually quantified in terms of an opening angle relative to the particle under consideration) are grouped together, and the collective effect of distant particles is averaged, possibly using a multipole expansion. This algorithm is originally due to \citet{barnes1986}, and octrees used for a gravitational force calculation are known as a Barnes-Hut tree. These two approaches can be combined to produce a TreePM algorithm \citep{bagla2002}, with a tree used to calculate the influence of nearby particles, and mesh deposition used for the most distant particles. This allows much easier implementation of periodic boundary conditions useful for cosmology simulations while retaining the high force resolution of the Barnes-Hut algorithm.

Including the effects of gas dynamics is critical for understanding many processes in astrophysics. Many problems include the coupled effects of particle dynamics and gas dynamics, so it becomes very appealing to directly include gas dynamics in an N-body model. \citet{lucy1977} and \citet{gingold1977} introduced the concept of smoothed particle hydrodynamics, where N-body particles are used as Lagrangian markers to sample an underlying continuous field. Rather than defining the properties of the gas based on local properties of any given particle, the properties of the gas are calculated based on a weighted average of the value of a field at the locations of nearby particles. The weighting function, known as the smoothing kernel, defines a finite compact region where particles may interact with each other. After calculating the gravitational forces (if any) using the N-body algorithms described above, the hydrodynamic forces may be calculated by solving a discretized version of the equiations of fluid dynamics, where the individual terms in the equations are calcualted by averaging over a smoothing kernel. See \citet{price2012} for a detailed recent review of modern SPH formalism.

In recent years, SPH algorithms have come under criticism for not being able to resolve certain fluid instabilities \citep{agertz2007} and artificially suppressing mixing \citep{read2010}. This has led to the development of alternative SPH formalisms \citep{abel2011, hopkins2013}, as well as moving mesh methods (see Section~\ref{usm} for more discussion).

\subsection{N-body and SPH Simulation Codes}

The Gadget family of SPH codes is quite possibly the most commonly used astrophysical simulation code. Originally created in 1998 by Volker Springel as part of his PhD project, Gadget-1 was released publicly in 2000 \citep{springel2001}. Later, the next-generation Gadget-2 code was released, including a complete rewrite in C, and improved algorithms, including an entropy-conserving approach for forumulating SPH, and a TreePM implementation \citep{springel2005b} that makes Gadget-2 suitable for large-scale N-body calculations of structure formation. This capability was used to run the Millenium simulations \citep{springel2005a}, at the time the largest pure N-body simulation of structure formation in the universe, using more than $10^{10}$ particles. Finally, Gadget-3 was developed afterwards but was never released publicly. It includes an improved domain decomposition algorithm. In addition, Gadget has been used extensively to study astrophysical phenomena at many scales, including the impact of gas on cosmological structure formation \citep{keres2009, schaye2015}, isolated galaxies \citep{springel2005c} and galaxy collisions \citep{robertson2006}, active galactic nucleus (AGN) feedback \citep{sijacki2007}, the formation of the first stars \citep{clark2011}, and simulations of the formation of the Moon \citep{jackson2012}.

Gadget-2 was released under the terms of the GNU General Public License v2 in 2005. It is available as a tarball at \url{http://wwwmpa.mpa-garching.mpg.de/gadget/}. The public release includes an adiabatic hydrodynamics module, self gravity based on a Barnes-Hut tree or TreePM, and can be used in comoving coordinates, making it suitable for cosmological simulations. The public release does not include any additional physics modules. While there were physics modules developed for the original version of the code, they were kept private. This lack of physics modules in the public version has led to the development of many independent private versions of Gadget-2 that include alternate implementations of physics such as cooling and heating processes, star formation and feedback, AGN feedback, and sink particles. Rather than any single Gadget community, there are instead balkanized islands of code development within individual research groups.

Tipsy (\url{https://github.com/N-BodyShop/tipsy}) is a paired data format and visualization tool originally written by Tom Quinn in the late 80's. The data format is shared by a number of simulation codes whose lineage traces back to the N-body shop research group at the University of Washington.

PKDGRAV \citep{stadel2001} is a pure N-body dynamics code. Rather than making use of an octree to calculate the gravitational force, it instead makes use of a parallel $k$-d tree. Compared with an octre, a $k$-d tree has better data balance and can be constructed more efficiently. This property enabled extremely high resolution simulations of galaxy formation in $\Lambda$-CDM cosmology, such as the {\it via lactea\/} simulations \citep{diemand2007, diemand2008}. More recently, PKDGRAV was ported to GPUs \citep{potter2017} and made publicly available (\url{https://bitbucket.org/dpotter/pkdgrav3/}), although without a license declaration. In addition, PKDGRAV is the basis of the other simulation codes in the Tipsy family, which include the effects of hydrodynamics via SPH particles.

Gasoline \citep{wadsley2004} is a privately developed SPH code originally built on top of PKDGRAV.\@ Similar to how PKDGRAV was used most impressively to elucidate the details of Milky Way mass dark matter halos, gasoline has been used quite heavily to understand the details of disk galaxy formation using high resolution simulations. An early example was published by \citet{governato2004}, although these models suffered spurious angular momentum loss due to poor resolution in the disk and inefficient feedback.  Later gasoline simulations employed higher resolution and improevd physical models, including the blastwave feedback model of \citet{stinson2006} and metal line cooling and metal mixing models of \citet{shen2010}, produced more realistic galaxy models, including the simulations described by \citet{guedes2011}, \citet{pontzen2012}, and \citet{zolotov2012}. More recently, the next generation version: Gasoline2 was described in \citet{wadsley2017}. This version includes an updated SPH formalism which eliminates many of the drawbacks of ``traditional'' SPH described above.

Recently, the physical models available in the Gasoline codebase were incorporated into the ChaNGA code \citep{jetley2008}, a research codebase built on top of the \texttt{Charm++} message passing library. Charm++ enables ChaNGa to scale to higher CPU counts and obtain improved efficiency by enabling asynchronous calculation of the various steps in the simulation timestep by the compute nodes. ChaNGa is publicaly available at \url{https://github.com/N-BodyShop/changa}. The public release of ChaNGa on GitHub does not include a license declaration and Charm++ has a license that forbids commercial use.

Other public clodes include SEREN ({\small \url{http://dhubber.github.io/seren}}) \citep{hubber2011} and PHANTOM ({\small \url{https://phantomsph.bitbucket.io/}}) \citep{price2017}. SEREN was released under the GPLv2. It has been heavily used in simulations of the interstellar medium and star cluster formation and includes prescriptions for sink particles \citep{walch2013} as well as timestepping schemes suitable for tracking N-body interactions in clusters. PHANTOM is available under the GPLv3 and includes an advanced implementation of the SPH formalism, MHD capabilities, and subgrid models useful for studying the ISM and galaxies.

\section{Adaptive Mesh Refinement}
\label{amr}

Eulerian meshes are able to better capture shocks and instabilities compared with traditional SPH codes. In addition, SPH codes have historically had a difficult time incorporating magnetic fields, while this is straightforward in an Eulerian formulation, In contexts where these processes are important Eulerian codes have thrived in astrophysics.

In an early example of an Eulerian code being used in astrophysics, \citet{boss1979} numerically investigated the 3D collapse of a non-axisymmetric rotating cloud of gas in an attempt to understand the formation of binary stars, making use of an Eulerian mesh with as few as $\sim 10^4$ zones. Later work using Eulerian methods focused on improving robustness, accuracy, and spatial resolution. In an effort to improve robustness and apply a single code to many different problems \citet{stone1992} presented the Zeus code. Zeus employs relatively simple algorithms, solving the coupled evolution of gas, magnetic fields, and gravity on a staggered mesh using a finite difference scheme. ZEUS can be thought of as an ancestor to modern astrophysics simulation codes in its aim to provide a platform for simulating a wide variety of phenomena, however other codes provided a basis for modern algorithms.

In particular, the development of the adaptive mesh refinement (AMR) technique paired with the development of techniques for tracking hydrodynamcis with second-order accurate shock-capturing techniques in the 1980's proved fundamental to the development of modern Eulerian codes used in astrophysics. \citet{berger1984} presented the earliest example of AMR, where the computation is allowed to proceed at multiple independent resolution levels. In the same year \citet{colella1984} presented the piecewise parabolic method (PPM) code, demonstrating a shock capturing code with improved accuracy and reduced artificial diffusivity at a fixed computational cost compared with other methods. Finally \citet{berger1989} described an AMR code employing PPM hydrodynamics. This code is the ancestor of many of the modern astrophysics codes described above.

An AMR code segments the computational domain into rectilinear subsets, normally referred to as blocks or grids. An individual grid defines a fixed resolution and may overlap with grids at higher or lower resolution levels. Generally, grids are nested, with the hierarchy of grids defining a tree data structure. The equations of hydrodynamics can be solved independently within each grid while filling boundary conditions from results on neighboring grids patches. The need to fill in boundary conditions in so-called ``ghost'' zones means that AMR methods incur numerical overhead compared with SPH codes, which do not need to track extra fluid elements for bookkeeping purposes.

\subsection{AMR Codes}

Boxlib \citep{zhang2016} and Chombo \citep{adams2015} are software frameworks for building AMR codes. They handle the bookkeeping for the grid data structure and common I/O and communication primitives. Individual codes implement solvers for various physics applications. Generally these codes do not aim for general applicability. Instead they are developed by individual research groups for a primary research problem. Chombo was forked from Boxlib in 1998 so these frameworks share many concepts. These two software lineages correspond to different research groups within the US national labs. More recently these efforst have conslidated into the AMReX framework (\url{https://amrex-codes.github.io/}), and future efforts will be in the context of AMReX. Examples of simulation codes implemented using these frameworks include Orion, PLUTO, Nyx, Maestro, and Castro codes, all of which have had substantial impact on the literature.

An early result from the Orion code \citep{howell2003} demonstrated the minimum resolution necessary to resolve the collapse of a self-gravitating cloud of gas  \citep{truelove1998}, a key process in many astrophysics simulations. Later Orion simulations included increasingly realistic physical effects for simulating the collapse of molecular clouds into prostars including non-ideal MHD and radiation transport \citep{myers2014, rosen2016}. The PLUTO code \citep{mignone2012} is publicly available (\url{http://plutocode.ph.unito.it/}) and includes a number of advanced physics packages, including support for MHD, relatavistic hydrodynamics (RHD), dissipative and radiative processes, and physical setups such as shearing boxes and non-cartesian coordinate systems. PLUTO supports static mesh hierarchies so it is particularly well suited for problems like disks that have roughly fixed geometries. Examples include simulations of massive star formation \citep{kuiper2010}, simulations of magnetized disk accretion \citep{lesur2014}, and simulations of gas in the galactic center environment \citep{burkert2012}. Nyx \citep{almgren2013} focuses on scalable large-scale cosmology simulations to aid in constraining observations of cosmological parameters \citep{sorini2016}. It is publicly available (\url{https://github.com/AMReX-Astro/Nyx}) and is publicly developed there. Finally, the Maestro and Castro codes are primarily used for the detailed modeling progenitors of supernmova explosions. Maestro \citep{nonoka2014} includes solvers tuned for low Mach number flows, while Castro \citep{almgren2010} includes shack capturing and capabilities for resolving highly compressible flows. Both include sophisticated radiation transport and nuclear burning capabilities. Both codes are currently developed in the open on Github (see \url{https://github.com/AMReX-Astro/MAESTRO} and \url{https://github.com/AMReX-Astro/Castro}).

Enzo \citep{bryan2014} is another example of a openly developed community astrophysics code. Originally developed by Greg Bryan for his PhD thesis, in subsequent years a variety of research groups worked on private, forked versions of the Enzo code. Later these disparate research groups elected to merge their divergent codebases and publicly release Enzo (\url{http://enzo-project.org}). To this day, Enzo aims to be a community-developed and maintained code, with code review and testing handled by a team of core developers, and new code contributions landing as people develop them for their research. In the Enzo community there is much more of an expectation that newly developed physical models and features get upstreamed to the public version for shared development. For this reason Enzo supports a wide variety of physical models, including several MHD solvers, radiation, chemistry, star formation and star formation feedback. Examples of major science results produced using the Enzo code include simulations of the first stars \citep{abel2002} and earliest galaxies \citep{wise2012}, simulations of Milky-Way analogue disk galaxies \citep{goldbaum2016}, and large-scale cosmological simulations \cite{xu2016}.

%% \subsection{FLASH}
%% \label{flash}

The FLASH code \citep{fryxell2000} is similarly intended as a community resource. Rather than in the Enzo model where the community develops the code, FLASH initially had funding for full-time developers and is to this day mainly developed in private at the University of Chicago, although community contributions are regularly made part of the FLASH release \citep{dubey2013}. Rather than being publicly available, access to the source code must be requested via a web form at the FLASH webiste (\url{http://flash.uchicago.edu}) after agreeing to cite the authors in any publication making use of the code. FLASH is a modular fortran codebase and includes a variety of problem types, hydrodynamics, MHD, and gravity solvers, as well as modules for nuclear burning and multispecies equations of state out of the box that can be used for a wide variety of applications. Applications of FLASH in the literature include simulations of interstellar turbulence \citep{federrath2013}, simulations of supernova progenitors \citep{couch2014}, and tidal disruption events \citep{guillochon2013}.

%% \subsection{Cactus}
%% \label{cactus}

In the numerical relativity community the Einstein Toolkit \citep{loffler2012} is the tool of choice for numerically solving the equations of general relatavistic magnetohydrodynamics (GRMHD) and Einstein's equations. The Einstein Toolkit is based on the Cactus framework and the Carpet AMR driver and is developed in the open as a community resource under the GPL \citep{loffler2013} (\url{https://bitbucket.org/einsteintoolkit/einsteintoolkit}) while the Cactus framework is distributed under the LGPL.\@  The most common scientific use case for the Einstein Toolkit is numerical simulations of compact objects, including supernova progenitors \citep{mosta2014} and particular compact object mergers in support of grtavitational wave observations \citep{ajith2012}. Numerical simulations of these events are generate model gravitational wave signals that are fit to observations to infer properties of the objects such as mass ratio and orbital properties.

Other open astrophysics simulation codes making use of AMR include RAMSES \citep{teyssier2002} (\url{https://bitbucket.org/rteyssie/ramses}) and Athena++, the latest versiopn of the Athena simulation code (\url{http://princetonuniversity.github.io/athena/}) \citep{stone2008}. RAMSES transitioned to open development on Bitbucket relatively recently, while Athena is still a private code with a public version including limited support for advanced physics modules.

\section{New Codes and Future Directions}
\label{usm}

The current batch of AMR and SPH codes have had much success, as described above. However, over the past decade we have seen some movement away from these codes in favor of new computational techniques and codes that are able to exploit new supercomputer architectures. These new codes are briefly described here.

One approach is the emerging use of moving mesh techniques. In this approach, which can be thought of as a synthesis of the AMR and SPH approaches, the equations of hydrodynamics are solved on a voronoi mesh generated by a set of lagrangian mesh-generating points. Like SPH particles, these points follow the fluid flow and naturally end up located in regions that need enhanced resolution. Like an AMR code, the equations of fluid dynamics are solved using finite volume mthods and are evaluated using similar techniques to the PPM method described above. By far the most prolific moving mesh code in terms of published scientific results is the AREPO code \citep{springel2010}. AREPO can be thought of as an evolution of the Gadget SPH code, making use of the same data format and graivtational force algorithm, but with a completely updated and reworked hydrodynamics module. AREPO is a private code, although it has already been used extensively in the literature. Another public moving mesh code is Shadowfax \citep{vandenbroucke2016} (\url{http://www.dwarfs.ugent.be/shadowfax/}), released under the GPLv3. Science results produced using moving mesh codes include the Illustris simulations \citep{vogelsberger2014}, a suite of large-scale N-body/hydrodynamics cosmology simulations, as well as simulations of other astrophysics phenomena such as AGN jets \citep{weinberger2017}.

%% \subsection{GIZMO}
%% \label{gizmo}

The GIZMO code \citep{hopkins2015} is also an evolutionary descendent of the Gadget code that makes use of an improved hydrodynamics formulation. Gizmo is not a moving mesh code. Instead, its hydrodynamics module can be thought of a a novel reformaulation of meshless hydrodynamcis that is fundamentally different from smoothed particle hydrodynamics. Compared with SPH, it exhibits improved capturing of shocks, instabilities, and mixing. The major science results produced using Gizmo come from the FIRE simulation suite \citep{hopkins2017}, which captures the formation of a Milky Way analogue disk galaxy in a cosmological context using the zoom-in technique. There is a public release of GIZMO under the GPLv2 license ({\small \url{http://www.tapir.caltech.edu/~phopkins/Site/GIZMO.html}}) but the public release does include the full complement of physics packages available in the private version of Gizmo. According to the GIZMO website, access to the private version is available upon request.

%% \subsection{GAMER}

Finally, I would like to finish by highlighting the GAMER code \citep{schive2010}, a block-structured AMR code written to maximize the efficiency of modern GPU-powered supercomputer architectures. GAMER includes a number of physics modules that make it suitable for a large number of astrophysics applications, including N-body particle dynamics, MHD, star formation and feedback, and chemistry. Due to its use of GPU accelerators, GAMER is also substantially faster than CPU-bound AMR codes like FLASH and Enzo. GAMER will soon be made publicly available in a manner suitable for use by the community.

\bibliography{I5.1.bib}

\end{document}
